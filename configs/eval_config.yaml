wandb:
  track: True
  run_name: eval_warp_lr=1e-5_mu=0.1
  project_name: WARP_imdb

model:
  generator_checkpoint: lvwerra/gpt2-imdb
  reward_model_checkpoint: reward_model/final_checkpoint
  save_path: trained_warp_model_lr=1e-5_mu=0.01

data:
  test_data_path: prompts_dataset_tokenized_test

training:
  device: "cuda"

generation_config:
  bos_token_id: 50256
  eos_token_id: 50256
  pad_token_id: 50256
  max_length: 64
  use_cache: true
  temperature: 0.7
  top_k: 0.0
  top_p: 1.0
  do_sample: true
  return_tensors: "pt"