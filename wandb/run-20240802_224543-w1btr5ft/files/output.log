  0%|                                                                                     | 0/1173 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/root/anaconda3/envs/allignment/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/root/anaconda3/envs/allignment/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
Could not estimate the number of tokens of the input, floating-point operations will not be computed







  4%|███                                                                         | 48/1173 [00:17<05:42,  3.29it/s]







  8%|██████                                                                      | 94/1173 [00:32<05:33,  3.24it/s]








 12%|█████████▎                                                                 | 146/1173 [00:48<05:14,  3.26it/s]








 17%|████████████▌                                                              | 197/1173 [01:04<05:16,  3.08it/s]








 21%|███████████████▉                                                           | 250/1173 [01:20<04:44,  3.24it/s]







 25%|██████████████████▊                                                        | 295/1173 [01:34<04:31,  3.23it/s]








 30%|██████████████████████▏                                                    | 347/1173 [01:50<04:15,  3.23it/s]








 34%|█████████████████████████▌                                                 | 399/1173 [02:06<03:57,  3.26it/s]








 38%|████████████████████████████▊                                              | 450/1173 [02:22<03:42,  3.24it/s]







 42%|███████████████████████████████▋                                           | 495/1173 [02:36<03:28,  3.25it/s]

 43%|███████████████████████████████▉                                           | 500/1173 [02:37<03:26,  3.25it/s]/root/anaconda3/envs/allignment/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/root/anaconda3/envs/allignment/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):







 47%|███████████████████████████████████                                        | 549/1173 [02:54<03:12,  3.25it/s]







 51%|██████████████████████████████████████                                     | 595/1173 [03:08<02:57,  3.26it/s]








 55%|█████████████████████████████████████████▎                                 | 647/1173 [03:24<02:41,  3.26it/s]








 60%|████████████████████████████████████████████▋                              | 699/1173 [03:40<02:26,  3.25it/s]








 64%|████████████████████████████████████████████████                           | 751/1173 [03:56<02:09,  3.25it/s]







 68%|██████████████████████████████████████████████████▉                        | 796/1173 [04:10<01:58,  3.18it/s]








 72%|██████████████████████████████████████████████████████▎                    | 849/1173 [04:27<01:39,  3.25it/s]








 77%|█████████████████████████████████████████████████████████▍                 | 899/1173 [04:42<01:23,  3.26it/s]







 81%|████████████████████████████████████████████████████████████▍              | 945/1173 [04:56<01:10,  3.25it/s]








 85%|███████████████████████████████████████████████████████████████▋           | 997/1173 [05:12<00:53,  3.26it/s]

 85%|███████████████████████████████████████████████████████████████           | 1000/1173 [05:13<00:53,  3.25it/s]/root/anaconda3/envs/allignment/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/root/anaconda3/envs/allignment/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):







 90%|██████████████████████████████████████████████████████████████████▏       | 1050/1173 [05:30<00:38,  3.22it/s]







 93%|█████████████████████████████████████████████████████████████████████     | 1095/1173 [05:44<00:25,  3.04it/s]








 98%|████████████████████████████████████████████████████████████████████████▍ | 1148/1173 [06:01<00:07,  3.26it/s]




100%|██████████████████████████████████████████████████████████████████████████| 1173/1173 [06:08<00:00,  3.47it/s]

100%|██████████████████████████████████████████████████████████████████████████| 1173/1173 [06:10<00:00,  3.17it/s]