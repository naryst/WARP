  0%|                                                                                       | 0/1173 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/root/anaconda3/envs/allignment/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/root/anaconda3/envs/allignment/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
Could not estimate the number of tokens of the input, floating-point operations will not be computed

















  4%|███▍                                                                          | 51/1173 [00:37<12:55,  1.45it/s]

















  9%|██████▌                                                                      | 100/1173 [01:11<12:21,  1.45it/s]

















 13%|█████████▊                                                                   | 150/1173 [01:46<11:47,  1.45it/s]


















 17%|█████████████▎                                                               | 202/1173 [02:22<11:11,  1.45it/s]

















 21%|████████████████▍                                                            | 251/1173 [02:56<10:37,  1.45it/s]

















 26%|███████████████████▋                                                         | 300/1173 [03:30<10:03,  1.45it/s]

















 30%|██████████████████████▉                                                      | 350/1173 [04:04<09:29,  1.45it/s]


















 34%|██████████████████████████▍                                                  | 402/1173 [04:40<08:52,  1.45it/s]

















 38%|█████████████████████████████▌                                               | 451/1173 [05:14<08:20,  1.44it/s]

















 43%|████████████████████████████████▊                                            | 500/1173 [05:48<07:45,  1.45it/s]
 43%|████████████████████████████████▊                                            | 500/1173 [05:48<07:45,  1.45it/s]/root/anaconda3/envs/allignment/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/root/anaconda3/envs/allignment/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):

















 47%|████████████████████████████████████▏                                        | 551/1173 [06:25<07:10,  1.45it/s]

















 51%|███████████████████████████████████████▍                                     | 600/1173 [06:59<06:35,  1.45it/s]

















 55%|██████████████████████████████████████████▌                                  | 649/1173 [07:33<06:02,  1.45it/s]

















 60%|█████████████████████████████████████████████▊                               | 698/1173 [08:07<05:28,  1.45it/s]


















 64%|█████████████████████████████████████████████████▏                           | 750/1173 [08:43<05:06,  1.38it/s]

















 68%|████████████████████████████████████████████████████▍                        | 799/1173 [09:17<04:20,  1.44it/s]

















 72%|███████████████████████████████████████████████████████▋                     | 848/1173 [09:51<03:46,  1.43it/s]

















 76%|██████████████████████████████████████████████████████████▉                  | 897/1173 [10:25<03:11,  1.44it/s]


















 81%|██████████████████████████████████████████████████████████████▎              | 949/1173 [11:01<02:35,  1.44it/s]

















 85%|█████████████████████████████████████████████████████████████████▌           | 999/1173 [11:35<02:00,  1.45it/s]

 85%|████████████████████████████████████████████████████████████████▊           | 1000/1173 [11:36<01:59,  1.45it/s]/root/anaconda3/envs/allignment/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/root/anaconda3/envs/allignment/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
















 89%|███████████████████████████████████████████████████████████████████▉        | 1049/1173 [12:11<01:25,  1.45it/s]

















 94%|███████████████████████████████████████████████████████████████████████▏    | 1098/1173 [12:45<00:51,  1.45it/s]


















 98%|██████████████████████████████████████████████████████████████████████████▌ | 1150/1173 [13:21<00:15,  1.44it/s]








100%|████████████████████████████████████████████████████████████████████████████| 1173/1173 [13:37<00:00,  1.61it/s]

100%|████████████████████████████████████████████████████████████████████████████| 1173/1173 [13:39<00:00,  1.43it/s]