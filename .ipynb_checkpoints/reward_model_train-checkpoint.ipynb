{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad76fef0-de80-480e-8795-3a8c70c2becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import datasets\n",
    "from trl import RewardTrainer, RewardConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ff1c0c-d01e-48dd-baea-43cced350607",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fcfbdef-111a-4282-8bc9-d944b8f005ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_from_disk('comment_pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6761b1b5-c369-4b46-b7b7-b56bb2b88c0f",
   "metadata": {},
   "source": [
    "# Reward trainer Должен иметь следующие фичи на входе \n",
    "* `input_ids_chosen`\n",
    "* `attention_mask_chosen`\n",
    "* `input_ids_rejected` \n",
    "* `attention_mask_rejected`\n",
    "\n",
    "**Chosen** в нашем случае - позитивный комментарий, **Rejected** - негативный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58fde760-ded8-449a-aac5-2961c555731e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a91b2366cf4f12befb1ad28db454d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/12500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_data(sample):\n",
    "    tokenized_pos = tokenizer(sample['positive_comment'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "    tokenized_neg = tokenizer(sample['negative_comment'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "    sample['input_ids_chosen'] = tokenized_pos['input_ids']\n",
    "    sample['attention_mask_chosen'] = tokenized_pos['attention_mask']\n",
    "    sample['input_ids_rejected'] = tokenized_neg['input_ids']\n",
    "    sample['attention_mask_rejected'] = tokenized_neg['attention_mask']\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "data = data.map(tokenize_data, batched=True, num_proc=16)\n",
    "data.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8844b715-2980-49be-81d9-ff2283c4926a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RewardConfig.__init__() got an unexpected keyword argument 'padding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define reward config\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m reward_config \u001b[38;5;241m=\u001b[39m \u001b[43mRewardConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Initialize the RewardTrainer\u001b[39;00m\n\u001b[1;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m RewardTrainer(\n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     21\u001b[0m     args\u001b[38;5;241m=\u001b[39mreward_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Add your own metric computation if needed\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: RewardConfig.__init__() got an unexpected keyword argument 'padding'"
     ]
    }
   ],
   "source": [
    "# Define reward config\n",
    "reward_config = RewardConfig(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=1e-5,\n",
    "    do_eval=False,\n",
    "    report_to='none',\n",
    "    max_length=512,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize the RewardTrainer\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=reward_config,\n",
    "    train_dataset=data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None,  # Add your own metric computation if needed\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignment",
   "language": "python",
   "name": "alignment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
